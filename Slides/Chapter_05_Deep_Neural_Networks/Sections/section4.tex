\begin{frame}{Problem: Over-fitting in a Neural Network}
	\begin{itemize}
		\item Why does over-fitting happen in a Neural network?
		\begin{itemize}
			\item too many free parameters
		\end{itemize}
	\end{itemize}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.4\textwidth]{Figs/section_4/overfitting.png}
        \caption{Over-fitting in a Neural Network \href{https://en.wikipedia.org/wiki/Overfitting}{source}}
    \end{figure}
\end{frame}

\begin{frame}{Solution 1: L1/L2 regularization}
    \begin{itemize}
        \item just like a classic regularizer
        \item sum the regularizer term for every layer weight
    \end{itemize}
    \vspace{0.2\textheight}
    \begin{equation*}
        \mathlarger{\mathlarger{\mathlarger{
        L = \frac{1}{N} \sum_{i=1}^{N} L(\phi(x_i), y_i) + \lambda \sum_{i,j,k} R(W_{j,k}^{(i)})
        }}}
    \end{equation*}
\end{frame}
\begin{frame}{Solution 1: L1/L2 regularization}
    \begin{itemize}
        \item L1/L2 regularizer functions (review)
    \end{itemize}
    \vspace{0.1\textheight}
    \begin{equation*}
        \mathlarger{\mathlarger{\mathlarger{
            L1: R(w) = \vert w\vert
        }}}
    \end{equation*}
    \begin{equation*}
        \mathlarger{\mathlarger{\mathlarger{
            L2: R(w) = w^2
        }}}
    \end{equation*}
    \begin{itemize}
        \item you can also combine the two different regularizers (Elastic net)
    \end{itemize}
    \vspace{0.1\textheight}
    \begin{equation*}
        \mathlarger{\mathlarger{\mathlarger{
        R(w) = \beta w^2 + \vert w\vert
        }}}
    \end{equation*}
\end{frame}

\begin{frame}{Solution 2: Early Stopping}
    \begin{itemize}
        \item Stop learning when the validation error is Minimum
    \end{itemize}
    \begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{Figs/Early Stopping.png}
	\caption{Early-Stopping diagram\href{https://medium.com/analytics-v7idhya/early-stopping-with-pytorch-to-restrain-your-model-from-overfitting-dce6de4081c5}{source}}
    \end{figure}
\end{frame}

\begin{frame}{Solution 3: Dropout}
	during training:
	\begin{itemize}
		\item at each iteration, every neuron has a probability P of being dropped out
		\begin{itemize}
			\item meaning it will be ignored
		\end{itemize}
		\item P := dropout rate
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics[height=0.4\textheight]{Figs/Dropout-after.png}
		\caption{Behavior of dropout at training time. \href{https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf}{Source}}
	\end{figure}
\end{frame}
\begin{frame}{Solution 3: Dropout}
	during testing:
	\begin{itemize}
		\item at each iteration, on average, 1-P percent of Neurons are active
		\begin{itemize}
			\item less neurons results in bigger weights
		\end{itemize}
		\item we need to normalize weights by 1-P
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics[height=0.4\textheight]{Figs/Dropout-before.png}
		\caption{Behavior of dropout at testing time. \href{https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf}{Source}}
	\end{figure}
\end{frame}
\begin{frame}{Solution 3: Dropout}
	pros
	\begin{itemize}
		\item the dependence between different neuron decreases
		\begin{itemize}
			\item neurons get more generalized
		\end{itemize}
	\end{itemize}
	cons
	\begin{itemize}
		\item model complexity increases
	\end{itemize}
\end{frame}
\begin{frame}{Solution 3: Dropout}
	practice tips:
	\begin{itemize}
		\item still overfitting -> increase dropout rate
		\item underfitting -> decrease dropout rate
		\item layersize -> dropout rate
		\item usually in practice, people will use dropout after the last hidden layer
	\end{itemize}
	
	\begin{figure}[H]
		\centering
		\includegraphics[height=0.4\textheight]{Figs/Dropout-before.png}
		\caption{Behavior of dropout at testing time. \href{https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf}{Source}}
	\end{figure}
\end{frame}



\begin{frame}{Problem: Vanishing/Exploding Gradients}
	// Todo
    \begin{itemize}
    	\item beginning of learning -> He/ELU
    	\item during learning -> still exists
    \end{itemize}
    
    \begin{figure}[H]
    	\centering
    	\includegraphics[height=0.4\textheight]{Figs/Dropout-after.png}
    	\caption{Behavior of dropout at training time. \href{https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf}{Source}}
    \end{figure}
\end{frame}

\begin{frame}{Solution: Batch Norm layer}
	\begin{itemize}
		\item for normalizing the data
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\textwidth]{Figs/section_4/batchnorm_2.jpg}
		\caption{where batchnorm layer is placed \href{https://gradientscience.org/batchnorm/}{source}}
	\end{figure}
\end{frame}
\begin{frame}{Solution: Batch Norm layer}
    during training:
    \begin{itemize}
    	\item zero-centers and normalizes a batch
    \end{itemize}
	\begin{align*}
		\mu_B &= \textup{average of $x_B^{(i)}$}\\
		\sigma_B^2 &= \textup{variance of $x_B^{(i)}$}\\
		\hat{x_B}^{(i)} &= \frac{x_B^{(i)} - \mu_B}{\sqrt{\sigma^2_B + \epsilon}}
	\end{align*}
	\begin{itemize}
		\item scales and shifts with two learnable parameters $\gamma, \beta$
	\end{itemize}
	\begin{align*}
		y_B^{(i)} &= \gamma \hat{x_B}^{(i)} + \beta
	\end{align*}
\end{frame}
\begin{frame}{Solution: Batch Norm layer}
	during testing:
	\begin{itemize}
		\item to zero-center and normalize the input, we need average and variance of the whole data
		\item that can be acquired during the learning
		\item therefore two more learnable parameters!
	\end{itemize}
	\begin{align*}
		\mu &= \textup{average of $x^{(i)}$}\\
		\sigma^2 &= \textup{variance of $x^{(i)}$}\\
	\end{align*}
\end{frame}
\begin{frame}{Solution: Batch Norm layer}
	\begin{itemize}
		\item the effect it has on data
	\end{itemize}
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.75\textwidth]{Figs/section_4/batchnorm_3.png}
		\caption{batchnorm effect \href{https://jsideas.net/batch_normalization/}{source}}
	\end{figure}
\end{frame}
\begin{frame}{Solution: Batch Norm layer}
	Pros
	\begin{itemize}
		\item vanishing gradient problem reduced
		\begin{itemize}
			\item can use even saturating activation functions (tanh, logistics, etc.)
		\end{itemize}
		\item much less sensitive to initial weight
		\item exploding gradient problem reduced
		\begin{itemize}
			\item being able to use longer learning rates , which speeds up learning
		\end{itemize}
		\item acts as a regularizer, too
		\begin{itemize}
			\item no need for other regularizer techniques (dropout, etc.)
		\end{itemize}
	\end{itemize}
\end{frame}
\begin{frame}{Solution: Batch Norm layer}
	Cons
	\begin{itemize}
		\item adds complexity to the model (more parameters and computations)
		\begin{itemize}
			\item after training, we can mix the layer with its previous one
		\end{itemize}
	\end{itemize}
	\begin{equation*}
		\begin{equation}
			\mathlarger{x'^{(i)} = W x^{(i)} + b}
		\end{equation}
		\begin{equation}
			\mathlarger{z^{(i)} = \frac{x'^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta}
		\end{equation}
		\begin{equation}
			\mathlarger{z^{(i)} = W' x^{(i)} + b'; W' = \frac{1}{\sqrt{\sigma^2 + \epsilon}} W, b' = \beta + \frac{b- \mu}{\sqrt{\sigma^2 + \epsilon}}}
		\end{equation}
	\end{equation*}
\end{frame}
\begin{frame}{Solution: Batch Norm layer}
	Examples
	\begin{itemize}
		\item todo. examples from the book?
	\end{itemize}
\end{frame}


% //////////////////


\begin{frame}{Regularization: Adding Noise}
\begin{itemize}
\item We've seen a common approach for regularization thus far:
\begin{itemize}
	\item \textbf{Training}: Add some kind of randomness ($z$) : $$y = f_W(x, z)$$
	\item \textbf{Testing}: Average out the randomness: $$y=f(x)=\mathbb{E}_z[f(x, z)] = \int p(z)f(x, z)dz$$
\end{itemize}
\pause
\item Adding noise is another way to prevent a neural network from overfitting on the training data.
In every iteration, a random noise is added to the outputs of the layer, preventing consequent layers from co-adapting too much to the outputs of this layer.
\end{itemize}
\end{frame}
