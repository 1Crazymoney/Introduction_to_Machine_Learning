\begin{frame}{Early Stopping}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{Images/Early Stopping.png}
	\caption{Early Stopping. \href{https://medium.com/analytics-vidhya/early-stopping-with-pytorch-to-restrain-your-model-from-overfitting-dce6de4081c5}{Source}}
	\label{fig:Early Stopping}
\end{figure}
\begin{itemize}
\item Stop training when accuracy on the validation set decreases (or loss increases). Or keep track of the model parameters that worked best on validation set.
\end{itemize}
\end{frame}


\begin{frame}{Regularization: Add term to loss}
\begin{equation*}
\mathlarger{\mathlarger{L = \frac{1}{N} \sum_{i=1}^{N} L(\phi(x_i), y_i) + \lambda R(W)}}
\end{equation*}
\vspace{20pt}


Common regularization terms:
\begin{tabular}{l@{\hspace{0.3\textwidth}}l}
L2 regularization & $R(W)=\sum_{k}\sum_l W_{k,l}^2$\\
L1 regularization & $R(W)=\sum_{k}\sum_l W_{k,l}^2$\\
Elastic net (L1 + L2) & $R(W)=\sum_{k}\sum_l \beta W_{k,l}^2 + \vert W_{k,l}^2\vert$\\
\end{tabular}
\end{frame}

\begin{frame}{Regularization: Dropout}
\begin{itemize}
	\item Randomly set some of neurons to zero in forward pass.
\end{itemize}
\begin{figure}[H]
\centering
\begin{subfigure}[b]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{Images/Dropout-before.png}
	\caption{Without using dropout}
	\label{fig:Dropout-before}
\end{subfigure}
\begin{subfigure}[b]{0.3\textwidth}
	\centering
	\includegraphics[width=\textwidth]{Images/Dropout-after.png}
	\caption{After using dropout.}
	\label{fig:Dropout-after}
\end{subfigure}
\caption{Behavior of dropout at training time. \href{https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf}{Source}}
\end{figure}
\end{frame}

\begin{frame}{Regularization: Dropout}
\noindent\begin{minipage}{0.4\textwidth}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{Images/Dropout-after.png}
	\caption{\href{https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf}{Source}}
\end{figure}
\end{minipage}
\noindent\begin{minipage}{0.4\textwidth}
\begin{itemize}
	\item Dropout:
	\begin{itemize}
		\item Prevents co-adaptation of features (forces network to have redundant representations).
		\item Can be considered a large ensemble of models sharing parameters.
	\end{itemize}
\end{itemize}
\end{minipage}
\end{frame}

\begin{frame}{Dropout: Test Time}
\begin{itemize}
\item Dropout makes output of network random!
\begin{center}
\begin{tabular}{l@{\hspace{0.25\textwidth}}l}
&$z$: random mask\\
$y = f_W(x, z)$
& $x$: input of the layer\\
& $y$: output of the layer\\
\end{tabular}
\end{center}
\item We want to "average out" the randomness at test time:
\begin{equation*}
y=f(x)=\mathbb{E}_z[f(x, z)] = \int p(z)f(x, z)dz
\end{equation*}

\item Can we calculate the integral exactly?\\
\pause
\item We need to approximate the integral.
\end{itemize}
\end{frame}

\begin{frame}{Dropout: Test Time}
\begin{itemize}
	\item At test time neurons are always present and its output is multiplied by dropout probability:
\end{itemize}
\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/dropout-test time-before.png}
		\caption{Without using dropout}
		\label{fig:dropout-test time-before}
	\end{subfigure}
	\begin{subfigure}[b]{0.3\textwidth}
		\centering
		\includegraphics[width=\textwidth]{Images/dropout-test time-after.png}
		\caption{After using dropout.}
		\label{fig:dropout-test time-after}
	\end{subfigure}
	\caption{Behavior of dropout at test time. \href{https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf}{Source}}
\end{figure}
\end{frame}


\begin{frame}{Regularization: Adding Noise}
\begin{itemize}
\item We've seen a common approach for regularization thus far:
\begin{itemize}
	\item \textbf{Training}: Add some kind of randomness ($z$) : $$y = f_W(x, z)$$
	\item \textbf{Testing}: Average out the randomness: $$y=f(x)=\mathbb{E}_z[f(x, z)] = \int p(z)f(x, z)dz$$
\end{itemize}
\pause
\item Adding noise is another way to prevent a neural network from overfitting on the training data.
In every iteration, a random noise is added to the outputs of the layer, preventing consequent layers from co-adapting too much to the outputs of this layer.
\end{itemize}
\end{frame}

\begin{frame}{Batch Normalization}
\begin{tabular}{l l}
Input: $x: N\times D$ & Learnable Parameters: $\gamma, \beta: D$\\
Output: $y: N\times D$ & Intermediates: $\mu, \sigma: D$, $\hat{x}: N \times D$
\end{tabular}
\begin{align*}
\mu_j &= \textup{(Running) average of values seen during training}\\
\sigma^2_j &= \textup{(Running) average of values seen during training}\\
\hat{x}_{i, j} &= \frac{x_{i, j} - \mu_j}{\sqrt{\sigma^2_j + \epsilon}}\\
y_{i,j} &= \gamma_j \hat{x}_{i, j} + \beta_j
\end{align*}
\end{frame}

\begin{frame}{Batch Normalization}
\begin{itemize}
\item Batch normalization is done along with \textbf{C} axis in convolutional networks:
\begin{figure}
	\centering
	\includegraphics[width=0.3\textwidth]{Images/batch normalization for cnn.png}
	\caption{Batch normalization in CNNs \href{https://arxiv.org/pdf/1803.08494.pdf}{Source}.}
	\label{fig:batch normalization for cnn}
\end{figure}
\begin{itemize}
\item BN for FCNs: $x, y: N\times D \rightarrow \mu, \sigma, \gamma, \beta: 1\times D$
\item BN for CNNs: $x, y: N\times C \times H \times W \rightarrow \mu, \sigma, \gamma, \beta: 1\times C \times 1 \times 1$
\item In both cases: $y = \gamma (x - \mu) / \sigma + \beta$
\end{itemize}
\end{itemize}

\end{frame}



